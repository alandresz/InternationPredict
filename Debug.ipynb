{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a7d3ee-0830-4222-aaf6-b2bcb5a30792",
   "metadata": {},
   "source": [
    "# Challenge DataIQ - Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ea343-b823-417e-bf4f-d51f695117a6",
   "metadata": {},
   "source": [
    "## Definir variables a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf7d8108-a243-4e23-9917-56bcf21987a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "## Definicion de variables a utilizar\n",
    "@dataclass\n",
    "class Config:\n",
    "\n",
    "    archivos=['Episodios_Diagnosticos.csv',     # Nombre de los archivos que contienen la información a procesar\n",
    "              'Estudios_Complementarios.csv',\n",
    "              'Pacientes.csv',\n",
    "              'Signos_Vitales.csv']\n",
    "\n",
    "    path = \"./data/\"    # Ruta donde se encuentran los archivos\n",
    "\n",
    "    n_charenc=10000   # cantidad de caracteres para evaluar el encoding de los archivos\n",
    "\n",
    "    features=['EDAD',                     # features a considerar para el modelo (chequear que los nombres correspondan a features en df_final)\n",
    "              'SEXO', \n",
    "              'CANTIDAD_EPISODIOS', \n",
    "              'CANTIDAD_ESTUDIOS', \n",
    "              'CANTIDAD_SIGNOS_VITALES',\n",
    "              'PRIMER_AREA_FRECUENTE',\n",
    "              'ULTIMO_AREA_FRECUENTE',\n",
    "              'TIPO_DIAGNOSTICO_FRECUENTE'] \n",
    "    \n",
    "    test_size=0.20     # proporcion del dataset de testing\n",
    "    seed=42            # semilla de randomizacion\n",
    "\n",
    "    model=\"Random Forest\"  # modelo a utilizar en el entranmiento y evaluacion \n",
    "    cv_split=10            # segmentacion para el proceso de cross-validation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb2eaed1-c331-45ae-beed-9447446e6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559c0c1-423d-4b17-a214-ff7e023dece8",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06706f6e-732b-4613-a4e3-3811c757b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Episodios_Diagnosticos.csv\n",
      "./data/\n",
      "Successfully read Episodios_Diagnosticos.csv with encoding: ISO-8859-1\n",
      "./data/Estudios_Complementarios.csv\n",
      "./data/\n",
      "Warning: Failed to read Estudios_Complementarios.csv with detected encoding (ascii). Error: 'ascii' codec can't decode byte 0xf3 in position 49959: ordinal not in range(128)\n",
      "Successfully read Estudios_Complementarios.csv with encoding: ISO-8859-1\n",
      "./data/Pacientes.csv\n",
      "./data/\n",
      "Successfully read Pacientes.csv with encoding: ISO-8859-1\n",
      "./data/Signos_Vitales.csv\n",
      "./data/\n",
      "Warning: Failed to read Signos_Vitales.csv with detected encoding (ascii). Error: 'ascii' codec can't decode byte 0xd0 in position 76541: ordinal not in range(128)\n",
      "Successfully read Signos_Vitales.csv with encoding: ISO-8859-1\n",
      "Total DataFrames loaded: 4\n"
     ]
    }
   ],
   "source": [
    "# Evaluacion, lectura y carga de datos\n",
    "\n",
    "from src.preprocessing import load_and_read_data\n",
    "dfs = load_and_read_data(file_names=config.archivos, path=config.path, n_charenc=config.n_charenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c116a900-bb80-4519-ab64-8e3a338addd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division de dataframes para cada archivo \n",
    "\n",
    "df_episodios = dfs[0]\n",
    "df_estudios = dfs[1]\n",
    "df_pacientes = dfs[2]\n",
    "df_signos = dfs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce6c21af-4429-420d-922b-2c59fead3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar 'CLASE' al df de Episodios en base a 'TIPO_EPISODIO' ('CLASE'=1 si 'TIPO_EPISODIO' = 'H',  'CLASE' = 0 en otro caso)\n",
    "\n",
    "from src.preprocessing import crear_tipo_episodio\n",
    "df_episodios = crear_tipo_episodio(df_episodios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c74e9b6-1dc7-448d-845d-b4e87c9cac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza inicial, especificada en la documentación: Eliminacion de registros con 'TIPO_EPISODIO'='*' en df_episodios, \n",
    "#                                                     Eliminación de registros ducplicados en df_pacientes,\n",
    "#                                                     Eliminacion de columno 'ID_ITEM' en df_estudios. \n",
    "\n",
    "from src.preprocessing import default_clean\n",
    "df_episodios, df_pacientes, df_estudios = default_clean(df_episodios, df_pacientes, df_estudios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f020a24d-0f27-4ea6-9ee8-0ca06fbb28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion del DataFrame final, que será el dataset que se dividirá en entrenamiento y testing. Consultar documentacion aparte sobre que criterios se consideraron. \n",
    "# 'create_final_dataframe()' en 'preprocessing.py' contiene ena explicacion parcial de la misma\n",
    "\n",
    "from src.preprocessing import create_final_dataframe\n",
    "df_final = create_final_dataframe(df_pacientes, df_episodios, df_signos, df_estudios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18d5a664-ad8f-48d4-9d41-b8491da7d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de registros es la correcta: total de  2635\n"
     ]
    }
   ],
   "source": [
    "# Revisión de la longitud del dataset. Dado que los registros que se utilizaran corresponden a pacientes,\n",
    "#  el df_final debe ser de la misma longitud que df_pacientes, ya que la informacion concatenada en el mismo es relativa a los pacientes. \n",
    "\n",
    "from src.preprocessing import check_final_data\n",
    "check_final_data(df_final,df_pacientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4816e0e-8a0d-4d6a-a336-7e8529ebd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraccion de los conjuntos de entrenamiento y testeo. Se seleccionan los features, la proporcion del set de test y\n",
    "# la semilla de randomizacion (todas en el archivo de configuracion)\n",
    "\n",
    "from src.preprocessing import prepare_test_train\n",
    "X_train, X_test, y_train, y_test = prepare_test_train(df_final,features=config.features, test_size=config.test_size, seed=config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67796b82-1978-4cdc-aa30-699aab204160",
   "metadata": {},
   "source": [
    "# Entrenamiento y evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "40ee4252-cf02-4aa8-b520-1c60f577ba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Accuracy  Precision    Recall  F1-Score       AUC\n",
      "1           Random Forest  0.912713   0.900552  0.853403  0.876344  0.962766\n",
      "6       Gradient Boosting  0.912713   0.896175  0.858639  0.877005  0.960865\n",
      "0     Logistic Regression  0.910816   0.900000  0.848168  0.873315  0.951306\n",
      "3           Decision Tree  0.895636   0.857895  0.853403  0.855643  0.886523\n",
      "5             Naive Bayes  0.886148   0.832487  0.858639  0.845361  0.935007\n",
      "4     K-Nearest Neighbors  0.850095   0.804348  0.774869  0.789333  0.885845\n",
      "2  Support Vector Machine  0.842505   0.772727  0.801047  0.786632  0.884271\n",
      "Best performance model in Accuracy is:  Random Forest\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento y evaluacion en una serie de modelos posibles. Eleccion del mejor modelo (en base unicamente a la precision)\n",
    "\n",
    "from src.modeling import evaluate_models\n",
    "df_evaluate_models, best_model = evaluate_models(X_train, X_test, y_train, y_test, test_size=config.test_size, random_state=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a9d3864b-c7f5-4cbb-aaef-38d68da8ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n"
     ]
    }
   ],
   "source": [
    "# Agrego el mejor modelo al archivo de configuracion\n",
    "\n",
    "config.model = best_model\n",
    "print(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb99cc6-fd33-4e5f-8be1-16ccefd0a1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6af981b8-5bba-4fa4-ad6c-737033d0f4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo utilizado:  Random Forest\n",
      "Exactitud del modelo: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Dado el mejor modelo en la evaluacion, re-entreno el mismo y lo re-evaluo\n",
    "\n",
    "from src.modeling import train_and_evaluate_model \n",
    "model, accuracy, precission, recall, f1, auc = train_and_evaluate_model(X_train, X_test, y_train, y_test, model=config.model, seed=config.seed)\n",
    "#model, accuracy=train_and_evaluate_model(X_train, X_test, y_train, y_test, model=\"Gradient Boosting\", seed=config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a75b65-4f16-4025-91f6-57413e1c4ded",
   "metadata": {},
   "source": [
    "OBS: Se podria haber obtenido directamente el objeto modelo a partir de 'evaluate_models()', \n",
    "pero he optado por realizar ésta segunda accion por separado en base al siguiente criterio:\n",
    "\n",
    "1) En esta instancia no es relevante optimizar la eficiencia\n",
    "2) El tamaño del dataset, y por lo tanto tambien el tiempo de entrenamiento de los modelos evaluados, no significan un consumo elevado de recursos en la instancia actual de devlopment (sí tendría un impacto en una posible instancia de produccion). \n",
    "3) En pos de la claridad del codigo en esta instancia resulta preferible no anidar demasiadas funciones, por lo tanto prefiero no reutilizar 'train_and_evaluate_model()' dentro de 'evaluate_models()'\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6ee64cd6-176e-440b-8d5b-b27ab24e08f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de los puntajes de validación cruzada: 0.92\n",
      "Matriz de Confusión:\n",
      "[[318  18]\n",
      " [ 28 163]]\n",
      "Matriz de confusión porcentual:\n",
      "[[94.64  5.36]\n",
      " [14.66 85.34]]\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       336\n",
      "           1       0.90      0.85      0.88       191\n",
      "\n",
      "    accuracy                           0.91       527\n",
      "   macro avg       0.91      0.90      0.90       527\n",
      "weighted avg       0.91      0.91      0.91       527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluacion de metricas tipicas para el modelo entrenado: cross-validation, matriz de confusion, recall, F1\n",
    "\n",
    "from src.evaluation import full_metrics_eval\n",
    "cv_score, y_pred, cm, cr=full_metrics_eval(model, X_train, y_train, X_test, y_test, cv_split=config.cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e697f9e-ad25-4def-8302-6a7d2cf4bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_alt_1 = ['EDAD',                     \n",
    "                  'SEXO', \n",
    "                  #'CANTIDAD_EPISODIOS', \n",
    "                  #'CANTIDAD_ESTUDIOS', \n",
    "                  #'CANTIDAD_SIGNOS_VITALES',\n",
    "                  'PRIMER_AREA_FRECUENTE',\n",
    "                  'ULTIMO_AREA_FRECUENTE',\n",
    "                  'TIPO_DIAGNOSTICO_FRECUENTE'\n",
    "                  ] \n",
    "\n",
    "features_alt_2 = ['EDAD',                     \n",
    "                  'SEXO', \n",
    "                  #'CANTIDAD_EPISODIOS', \n",
    "                  #'CANTIDAD_ESTUDIOS', \n",
    "                  #'CANTIDAD_SIGNOS_VITALES',\n",
    "                  #'PRIMER_AREA_FRECUENTE',\n",
    "                  'ULTIMO_AREA_FRECUENTE',\n",
    "                  'TIPO_DIAGNOSTICO_FRECUENTE'\n",
    "                  ]\n",
    "\n",
    "features_alt_3 = ['EDAD',                     \n",
    "                  'SEXO', \n",
    "                  'CANTIDAD_EPISODIOS', \n",
    "                  'CANTIDAD_ESTUDIOS', \n",
    "                  'CANTIDAD_SIGNOS_VITALES',\n",
    "                  'PRIMER_AREA_FRECUENTE',\n",
    "                  'ULTIMO_AREA_FRECUENTE',\n",
    "                  'TIPO_DIAGNOSTICO_FRECUENTE'\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d4ee2e28-9117-4087-b4f0-84f0c2756789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Accuracy  Precision    Recall  F1-Score       AUC\n",
      "0     Logistic Regression  0.905123   0.907514  0.821990  0.862637  0.936986\n",
      "6       Gradient Boosting  0.891841   0.885057  0.806283  0.843836  0.940718\n",
      "3           Decision Tree  0.889943   0.863388  0.827225  0.844920  0.886507\n",
      "1           Random Forest  0.886148   0.846561  0.837696  0.842105  0.936986\n",
      "5             Naive Bayes  0.884250   0.831633  0.853403  0.842377  0.931626\n",
      "4     K-Nearest Neighbors  0.867173   0.834254  0.790576  0.811828  0.902214\n",
      "2  Support Vector Machine  0.815939   0.709821  0.832461  0.766265  0.883009\n",
      "Best performance model in Accuracy is:  Logistic Regression\n",
      "Modelo utilizado:  Logistic Regression\n",
      "Exactitud del modelo: 0.90\n",
      "Promedio de los puntajes de validación cruzada: 0.91\n",
      "Matriz de Confusión:\n",
      "[[320  16]\n",
      " [ 36 155]]\n",
      "Matriz de confusión porcentual:\n",
      "[[95.24  4.76]\n",
      " [18.85 81.15]]\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92       336\n",
      "           1       0.91      0.81      0.86       191\n",
      "\n",
      "    accuracy                           0.90       527\n",
      "   macro avg       0.90      0.88      0.89       527\n",
      "weighted avg       0.90      0.90      0.90       527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing import prepare_test_train\n",
    "X_train, X_test, y_train, y_test = prepare_test_train(df_final,features=features_alt_1, test_size=config.test_size, seed=config.seed)\n",
    "df_evaluate_models, best_model = evaluate_models(X_train, X_test, y_train, y_test, test_size=config.test_size, random_state=config.seed)\n",
    "config.model = best_model\n",
    "model, accuracy, precission, recall, f1, auc = train_and_evaluate_model(X_train, X_test, y_train, y_test, model=config.model, seed=config.seed)\n",
    "cv_score, y_pred, cm, cr=full_metrics_eval(model, X_train, y_train, X_test, y_test, cv_split=config.cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1298a2a9-89d3-4d69-a385-f3b42e4b7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Accuracy  Precision    Recall  F1-Score       AUC\n",
      "0     Logistic Regression  0.905123   0.907514  0.821990  0.862637  0.938061\n",
      "5             Naive Bayes  0.901328   0.897143  0.821990  0.857923  0.927574\n",
      "3           Decision Tree  0.897533   0.878453  0.832461  0.854839  0.897454\n",
      "6       Gradient Boosting  0.895636   0.890805  0.811518  0.849315  0.943211\n",
      "1           Random Forest  0.893738   0.857143  0.848168  0.852632  0.937407\n",
      "4     K-Nearest Neighbors  0.865275   0.848837  0.764398  0.804408  0.892569\n",
      "2  Support Vector Machine  0.814042   0.706667  0.832461  0.764423  0.871284\n",
      "Best performance model in Accuracy is:  Logistic Regression\n",
      "Modelo utilizado:  Logistic Regression\n",
      "Exactitud del modelo: 0.90\n",
      "Promedio de los puntajes de validación cruzada: 0.91\n",
      "Matriz de Confusión:\n",
      "[[321  15]\n",
      " [ 37 154]]\n",
      "Matriz de confusión porcentual:\n",
      "[[95.54  4.46]\n",
      " [19.37 80.63]]\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       336\n",
      "           1       0.91      0.81      0.86       191\n",
      "\n",
      "    accuracy                           0.90       527\n",
      "   macro avg       0.90      0.88      0.89       527\n",
      "weighted avg       0.90      0.90      0.90       527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_test_train(df_final,features=features_alt_2, test_size=config.test_size, seed=config.seed)\n",
    "df_evaluate_models, best_model = evaluate_models(X_train, X_test, y_train, y_test, test_size=config.test_size, random_state=config.seed)\n",
    "config.model = best_model\n",
    "model, accuracy, precission, recall, f1, auc = train_and_evaluate_model(X_train, X_test, y_train, y_test, model=config.model, seed=config.seed)\n",
    "cv_score, y_pred, cm, cr=full_metrics_eval(model, X_train, y_train, X_test, y_test, cv_split=config.cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d18eef1-d4c4-4e36-9935-9b0627e33929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Accuracy  Precision    Recall  F1-Score       AUC\n",
      "1           Random Forest  0.912713   0.900552  0.853403  0.876344  0.962766\n",
      "6       Gradient Boosting  0.912713   0.896175  0.858639  0.877005  0.960865\n",
      "0     Logistic Regression  0.910816   0.900000  0.848168  0.873315  0.951306\n",
      "3           Decision Tree  0.895636   0.857895  0.853403  0.855643  0.886523\n",
      "5             Naive Bayes  0.886148   0.832487  0.858639  0.845361  0.935007\n",
      "4     K-Nearest Neighbors  0.850095   0.804348  0.774869  0.789333  0.885845\n",
      "2  Support Vector Machine  0.842505   0.772727  0.801047  0.786632  0.884271\n",
      "Best performance model in Accuracy is:  Random Forest\n",
      "Modelo utilizado:  Random Forest\n",
      "Exactitud del modelo: 0.91\n",
      "Promedio de los puntajes de validación cruzada: 0.92\n",
      "Matriz de Confusión:\n",
      "[[318  18]\n",
      " [ 28 163]]\n",
      "Matriz de confusión porcentual:\n",
      "[[94.64  5.36]\n",
      " [14.66 85.34]]\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93       336\n",
      "           1       0.90      0.85      0.88       191\n",
      "\n",
      "    accuracy                           0.91       527\n",
      "   macro avg       0.91      0.90      0.90       527\n",
      "weighted avg       0.91      0.91      0.91       527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_test_train(df_final,features=features_alt_3, test_size=config.test_size, seed=config.seed)\n",
    "df_evaluate_models, best_model = evaluate_models(X_train, X_test, y_train, y_test, test_size=config.test_size, random_state=config.seed)\n",
    "config.model = best_model\n",
    "model, accuracy, precission, recall, f1, auc = train_and_evaluate_model(X_train, X_test, y_train, y_test, model=config.model, seed=config.seed)\n",
    "cv_score, y_pred, cm, cr=full_metrics_eval(model, X_train, y_train, X_test, y_test, cv_split=config.cv_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178d4bc-f330-44ba-9ded-f1654cabc3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de62287a-115e-43a3-a74f-780efc3b1ac2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model_with_metadata\n\u001b[0;32m      3\u001b[0m save_model_with_metadata(model, \n\u001b[1;32m----> 4\u001b[0m                          model_name, \n\u001b[0;32m      5\u001b[0m                          metrics, \n\u001b[0;32m      6\u001b[0m                          cv_score, \n\u001b[0;32m      7\u001b[0m                          classification_report, \n\u001b[0;32m      8\u001b[0m                          config, \n\u001b[0;32m      9\u001b[0m                          model_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     10\u001b[0m                          log_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_log.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "from src.evaluation import save_model_with_metadata\n",
    "\n",
    "save_model_with_metadata(model, \n",
    "                         model_name, \n",
    "                         metrics, \n",
    "                         cv_score, \n",
    "                         classification_report, \n",
    "                         config, \n",
    "                         model_dir=\"models\", \n",
    "                         log_file=\"model_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc4e2e-c0b4-4b54-b948-99149afb57c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
